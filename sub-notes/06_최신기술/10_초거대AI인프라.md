# 초거대 AI 인프라 아키텍처

## 1. 초거대 AI 인프라 정의

과기정통부 국가 AI 컴퓨팅 센터 구축 계획에 따라, Llama 3 70B, GPT-4 급 초거대 LLM을 학습하고 서빙하기 위한 GPU 클러스터 기반 분산 컴퓨팅 인프라. vLLM PagedAttention, 모델 병렬화, 고속 네트워크(InfiniBand)를 통해 대규모 AI 워크로드 처리

## 2. 초거대 AI 인프라 설명

### 1) GPU 클러스터 아키텍처 (그림)

```
┌─────────────────────────────────────────────────────────┐
│              스토리지 계층 (Shared FS)                   │
│         Lustre/GPFS - 100 PB, 1 TB/s                    │
└─────────────────┬───────────────────────────────────────┘
                  │ NVMe-oF
    ┌─────────────┼─────────────┐
    ↓             ↓             ↓
┌─────────┐ ┌─────────┐ ┌─────────┐
│ Node 1  │ │ Node 2  │ │ Node N  │  (512 노드)
│ 8×H100  │ │ 8×H100  │ │ 8×H100  │
│ 640GB   │ │ 640GB   │ │ 640GB   │
└────┬────┘ └────┬────┘ └────┬────┘
     │           │           │
     └───────────┼───────────┘
        InfiniBand NDR 400Gb/s (지연시간 < 1μs)
                 ↓
         ┌───────────────┐
         │  학습 계층     │
         │  DeepSpeed    │
         │  Megatron-LM  │
         └───────┬───────┘
                 ↓
         ┌───────────────┐
         │  서빙 계층     │
         │  vLLM         │
         │  TensorRT-LLM │
         └───────────────┘
```

- 간글: H100 GPU 4,096개 (512노드 × 8GPU) 구성. InfiniBand NDR 400Gb/s로 GPU 간 통신 병목 제거. 학습은 DeepSpeed, 서빙은 vLLM PagedAttention으로 메모리 효율 극대화

### 2) 모델 병렬화 전략 (표)

| 구분 | 세부 항목 | 설명 |
|------|-----------|------|
| **데이터 병렬화<br>(DP)** | 개념 | 각 GPU에 모델 전체 복사, 데이터만 분할 |
| | 적용 | Llama 3 8B (단일 GPU 가능) |
| | 한계 | 모델 > GPU 메모리 시 불가 |
| **텐서 병렬화<br>(TP)** | 개념 | 레이어 내부를 GPU에 분할 (Attention, FFN) |
| | 적용 | Llama 3 70B → 8-way TP (H100 8개) |
| | 통신량 | All-Reduce 빈번, InfiniBand 필수 |
| **파이프라인<br>병렬화 (PP)** | 개념 | 레이어를 GPU에 순차 배치, 마이크로배치 |
| | 적용 | GPT-3 175B → 64-way PP |
| | Bubble | 파이프라인 지연, 효율 80~90% |
| **3D 병렬화** | DP+TP+PP | Megatron-LM: DP=64, TP=8, PP=8 |
| | 확장성 | 수천 GPU 학습 가능 (GPT-4 급) |
| | 구현 | DeepSpeed ZeRO-3, FSDP |

- 간글: TP는 레이어 내부 분할로 통신 많지만 메모리 효율 최고. PP는 레이어 단위 분할로 통신 적지만 Bubble 존재. 3D 병렬화로 두 장점 결합, 대규모 학습 가능

### 3) LLM 서빙 최적화 기법 (표)

| 구분 | 세부 항목 | 설명 |
|------|-----------|------|
| **vLLM** | PagedAttention | KV Cache 페이징, 메모리 효율 2~4배 |
| | Continuous Batching | 토큰 단위 배치, 처리량 10배 향상 |
| | 성능 | HuggingFace 대비 처리량 25배 |
| **양자화** | INT8/INT4 | FP16 대비 메모리 1/2~1/4 절감 |
| | GPTQ, AWQ | 정확도 유지하며 압축 (Perplexity < 1%) |
| | 활용 | Llama 3 70B → INT4 = 35GB (H100 1개 가능) |
| **FlashAttention** | 알고리즘 | Tiling으로 HBM 접근 최소화 |
| | 효과 | Attention 연산 속도 2~4배, 메모리 절약 |
| | 지원 | vLLM, TensorRT-LLM 기본 탑재 |
| **스펙추레이션<br>디코딩** | 개념 | 작은 모델로 미리 예측 후 큰 모델 검증 |
| | 효과 | 토큰 생성 속도 2~3배 향상 |
| | 예시 | Llama 3 8B → 70B 검증 |

- 간글: PagedAttention으로 동적 배치 크기 처리, 메모리 낭비 제거. 양자화로 추론 비용 1/4 감소. FlashAttention은 긴 컨텍스트(128K 토큰) 처리 필수 기술

## 3. 성능 분석 및 구축 방안

### 1) 벤치마크 결과
- **학습 성능**: Llama 3 70B 학습 시간 30일 → 7일 (512×H100)
- **서빙 성능**: vLLM 처리량 1,200 tok/s (HuggingFace 340 tok/s)
- **비용 효율**: 양자화 적용 시 GPU 1/4 절감 → TCO 60% 감소

### 2) 구축 방안
- **네트워크**: InfiniBand NDR 400Gb/s, Fat-Tree 토폴로지
- **스토리지**: Lustre 병렬 파일시스템, NVMe-oF 100 PB
- **쿨링**: Liquid Cooling (H100 발열 700W × 4,096 = 2.8 MW)
- **소프트웨어**: Slurm 스케줄러, NCCL 2.18+, CUDA 12.0+

### 3) 운영 과제
- **에너지**: 3 MW 전력 소비, 탄소중립 고려 (태양광 연계)
- **장애 대응**: GPU 고장률 연 5%, 체크포인팅 필수
- **보안**: 모델 IP 유출 방지, 접근 제어 (Zero Trust)
